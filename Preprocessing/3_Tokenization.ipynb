{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Tokenization & Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Social Media Post</th>\n",
       "      <th>Normalized Claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>president \"biden's plan would mean america's s...</td>\n",
       "      <td>biden s energy plan would get rid of seniors a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>important announcement coronavirus last evenin...</td>\n",
       "      <td>if someone with the new coronavirus sneezes, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>heart is delighted to hear</td>\n",
       "      <td>heart is delighted to hear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>an allowed appeal is one where the initial ref...</td>\n",
       "      <td>the vast majority of people coming across the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>warm water therapy dr. d. mensah asare says th...</td>\n",
       "      <td>a widely popular social media post claims that...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PID                                  Social Media Post  \\\n",
       "0    1  president \"biden's plan would mean america's s...   \n",
       "1    2  important announcement coronavirus last evenin...   \n",
       "2    3                         heart is delighted to hear   \n",
       "3    4  an allowed appeal is one where the initial ref...   \n",
       "4    5  warm water therapy dr. d. mensah asare says th...   \n",
       "\n",
       "                                    Normalized Claim  \n",
       "0  biden s energy plan would get rid of seniors a...  \n",
       "1  if someone with the new coronavirus sneezes, i...  \n",
       "2                         heart is delighted to hear  \n",
       "3  the vast majority of people coming across the ...  \n",
       "4  a widely popular social media post claims that...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"CLAN_data_cleaned.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will Use Hugging Face Tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Social Media Post</th>\n",
       "      <th>Normalized Claim</th>\n",
       "      <th>Tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>president \"biden's plan would mean america's s...</td>\n",
       "      <td>biden s energy plan would get rid of seniors a...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>important announcement coronavirus last evenin...</td>\n",
       "      <td>if someone with the new coronavirus sneezes, i...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>heart is delighted to hear</td>\n",
       "      <td>heart is delighted to hear</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>an allowed appeal is one where the initial ref...</td>\n",
       "      <td>the vast majority of people coming across the ...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>warm water therapy dr. d. mensah asare says th...</td>\n",
       "      <td>a widely popular social media post claims that...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>2798</td>\n",
       "      <td>foh with that weak malala slander, twitter fin...</td>\n",
       "      <td>malala yousafzai has been silent about the cri...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>2802</td>\n",
       "      <td>the plight of hindu girls madly in love with m...</td>\n",
       "      <td>the plight of hindu girls madly in love with m...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>2804</td>\n",
       "      <td>i am hearing that james comey has 50 counts of...</td>\n",
       "      <td>james comey has 50 counts of treason. and john...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>2806</td>\n",
       "      <td>dr. lonnie herman reviews how your food is pre...</td>\n",
       "      <td>irradiated food causes cancer</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>2809</td>\n",
       "      <td>four weeks ago, all uk primary aged children g...</td>\n",
       "      <td>nasal flu vaccines given to children contain s...</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2290 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PID                                  Social Media Post  \\\n",
       "0        1  president \"biden's plan would mean america's s...   \n",
       "1        2  important announcement coronavirus last evenin...   \n",
       "2        3                         heart is delighted to hear   \n",
       "3        4  an allowed appeal is one where the initial ref...   \n",
       "4        5  warm water therapy dr. d. mensah asare says th...   \n",
       "...    ...                                                ...   \n",
       "2285  2798  foh with that weak malala slander, twitter fin...   \n",
       "2286  2802  the plight of hindu girls madly in love with m...   \n",
       "2287  2804  i am hearing that james comey has 50 counts of...   \n",
       "2288  2806  dr. lonnie herman reviews how your food is pre...   \n",
       "2289  2809  four weeks ago, all uk primary aged children g...   \n",
       "\n",
       "                                       Normalized Claim  \\\n",
       "0     biden s energy plan would get rid of seniors a...   \n",
       "1     if someone with the new coronavirus sneezes, i...   \n",
       "2                            heart is delighted to hear   \n",
       "3     the vast majority of people coming across the ...   \n",
       "4     a widely popular social media post claims that...   \n",
       "...                                                 ...   \n",
       "2285  malala yousafzai has been silent about the cri...   \n",
       "2286  the plight of hindu girls madly in love with m...   \n",
       "2287  james comey has 50 counts of treason. and john...   \n",
       "2288                      irradiated food causes cancer   \n",
       "2289  nasal flu vaccines given to children contain s...   \n",
       "\n",
       "                        Tokenized  \n",
       "0     [input_ids, attention_mask]  \n",
       "1     [input_ids, attention_mask]  \n",
       "2     [input_ids, attention_mask]  \n",
       "3     [input_ids, attention_mask]  \n",
       "4     [input_ids, attention_mask]  \n",
       "...                           ...  \n",
       "2285  [input_ids, attention_mask]  \n",
       "2286  [input_ids, attention_mask]  \n",
       "2287  [input_ids, attention_mask]  \n",
       "2288  [input_ids, attention_mask]  \n",
       "2289  [input_ids, attention_mask]  \n",
       "\n",
       "[2290 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pretrained tokenizer (choose based on your model)\n",
    "MODEL_NAME = \"facebook/bart-base\"  # Change this if using BERT or T5\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Ensure all entries are strings\n",
    "df[\"Social Media Post\"] = df[\"Social Media Post\"].astype(str)\n",
    "\n",
    "# Tokenize the Social Media Posts\n",
    "df[\"Tokenized\"] = df[\"Social Media Post\"].apply(\n",
    "    lambda x: tokenizer(\n",
    "        x, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Check tokenized output\n",
    "df[\"Tokenized\"].head()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion of Tokenized Data in Model Input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input shape: torch.Size([2290, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Ensure labels are strings\n",
    "df[\"Normalized Claim\"] = df[\"Normalized Claim\"].astype(str)\n",
    "\n",
    "# Encode Labels (if required for classification)\n",
    "# If this is a text generation task, use tokenizer on Normalized Claim as well\n",
    "labels = tokenizer(\n",
    "    df[\"Normalized Claim\"].tolist(),\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Tokenized input shape:\", labels[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   428, 12145,  ...,     1,     1,     1],\n",
       "        [    0,  1594,   951,  ...,     1,     1,     1],\n",
       "        [    0, 12690,    16,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,   267, 12336,  ...,     1,     1,     1],\n",
       "        [    0,   853,  7822,  ...,     1,     1,     1],\n",
       "        [    0, 22118,   337,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Data Loader Class for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   347,  1417,   808,    12,  1646,    16,    10, 23195,   328,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   846,  7904,  3141,  5585,  5177,   611,  7418,     4,     2,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[    0,   347,  1417,   808,    12,  1646,    16,    45,    10, 23195,\n",
      "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   846,  7904,  3141,   109,    45,  5585,  5177,   611,  7418,\n",
      "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer (Modify model name if needed)\n",
    "MODEL_NAME = \"facebook/bart-base\"  # Change to \"t5-small\" if using T5\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "class ClaimDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list): List of input text strings.\n",
    "            labels (list): List of target labels (normalized claims).\n",
    "            tokenizer (AutoTokenizer): Pretrained tokenizer.\n",
    "            max_length (int): Max token length.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Tokenizes the input text and label, and returns tensors.\n",
    "        \"\"\"\n",
    "        text_encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        label_encoding = self.tokenizer(\n",
    "            self.labels[idx],  # Labels should also be tokenized\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": text_encoding[\"input_ids\"].squeeze(\n",
    "                0\n",
    "            ),  # (1, max_length) → (max_length,)\n",
    "            \"attention_mask\": text_encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": label_encoding[\"input_ids\"].squeeze(\n",
    "                0\n",
    "            ),  # Labels must be tokenized for seq2seq\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to stack tensors properly.\n",
    "    \"\"\"\n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n",
    "    attention_mask = torch.stack([b[\"attention_mask\"] for b in batch])\n",
    "    labels = torch.stack([b[\"labels\"] for b in batch])\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "\n",
    "# Example Data (Replace with actual dataset)\n",
    "texts = [\"Covid-19 is a hoax!\", \"Vaccines contain microchips.\"]\n",
    "labels = [\"Covid-19 is not a hoax.\", \"Vaccines do not contain microchips.\"]\n",
    "\n",
    "# Create Dataset\n",
    "dataset = ClaimDataset(texts, labels, tokenizer)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Check a Sample Batch\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
