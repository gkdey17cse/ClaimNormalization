{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58fe7787-3100-4daf-ac7f-c8d7dfdf8ff6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T22:03:50.636445Z",
     "iopub.status.busy": "2025-04-08T22:03:50.636127Z",
     "iopub.status.idle": "2025-04-08T22:03:51.014345Z",
     "shell.execute_reply": "2025-04-08T22:03:51.013461Z",
     "shell.execute_reply.started": "2025-04-08T22:03:50.636419Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/clan-complete/test_data_cleaned.csv\n",
      "/kaggle/input/clan-complete/CLAN_data.csv\n",
      "/kaggle/input/clan-complete/CLAN_data_cleaned_test.csv\n",
      "/kaggle/input/clan-complete/CLAN_data_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d851bc2-6fdb-4a2d-922a-41f2a65f5d34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T22:16:37.548702Z",
     "iopub.status.busy": "2025-04-08T22:16:37.548284Z",
     "iopub.status.idle": "2025-04-08T22:16:37.712133Z",
     "shell.execute_reply": "2025-04-08T22:16:37.711140Z",
     "shell.execute_reply.started": "2025-04-08T22:16:37.548671Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open /kaggle/input/your-dataset-name/t5_clan_model.zip, /kaggle/input/your-dataset-name/t5_clan_model.zip.zip or /kaggle/input/your-dataset-name/t5_clan_model.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!unzip -q \"/kaggle/input/your-dataset-name/t5_clan_model.zip\" -d \"/kaggle/working/Model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ed4b1-5a3d-4293-b3b5-47af81dffb2d",
   "metadata": {},
   "source": [
    "# All Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43e59c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T22:04:00.373125Z",
     "iopub.status.busy": "2025-04-08T22:04:00.372652Z",
     "iopub.status.idle": "2025-04-08T22:04:14.745738Z",
     "shell.execute_reply": "2025-04-08T22:04:14.744378Z",
     "shell.execute_reply.started": "2025-04-08T22:04:00.373101Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.51.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
      "Requirement already satisfied: bert-score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.5.1)\n",
      "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.5.1+cu121)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.51.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers datasets sentencepiece evaluate bert-score rouge-score matplotlib nltk sacrebleu contractions\n",
    "!pip install --upgrade transformers\n",
    "!pip install transformers datasets evaluate rouge-score sacrebleu bert-score --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df59547-69c4-4bcb-bd0b-fd31e089d52b",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec04500",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Remove twitter handles, URLs, HTML tags\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)                     # Remove Twitter handles\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)           # Remove URLs\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)                    # Remove HTML tags\n",
    "\n",
    "    # Remove quotes but keep the content\n",
    "    text = text.replace('\"', \"\").replace(\"“\", \"\").replace(\"”\", \"\")\n",
    "\n",
    "    # Replace hashtags with just the word\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
    "\n",
    "    # Remove newlines, tabs, extra whitespace\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Lowercase everything\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove non-informative punctuation (preserve . , ! ? for structure if needed)\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation.replace('.', '').replace(',', '').replace('!', '').replace('?', ''))}]\", \"\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_clan_raw_data(input_path, output_path):\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    # Keep only necessary columns\n",
    "    df = df[[\"PID\", \"Social Media Post\", \"Normalized Claim\"]]\n",
    "\n",
    "    # Drop rows with missing data\n",
    "    df.dropna(subset=[\"Social Media Post\", \"Normalized Claim\"], inplace=True)\n",
    "\n",
    "    # Clean both post and normalized claim\n",
    "    df[\"Social Media Post\"] = df[\"Social Media Post\"].apply(clean_text)\n",
    "    df[\"Normalized Claim\"] = df[\"Normalized Claim\"].apply(clean_text)\n",
    "\n",
    "    # Drop rows where cleaned content is now empty\n",
    "    df = df[(df[\"Social Media Post\"].str.strip() != \"\") & (df[\"Normalized Claim\"].str.strip() != \"\")]\n",
    "\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(subset=[\"Social Media Post\", \"Normalized Claim\"], inplace=True)\n",
    "\n",
    "    # Save cleaned dataset\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Cleaned data saved to {output_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "cleaned_df = preprocess_clan_raw_data(\"/kaggle/input/clan-complete/CLAN_data.csv\", \"/kaggle/working//CLAN_data_processed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fba65e-6e80-4ea1-a4aa-3315985c98c8",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# For testing\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/kaggle/working/test_data_cleaned.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c80c30",
   "metadata": {},
   "source": [
    "# Code to Train BART-BASE Piepeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5256c8",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "from bert_score import score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "df = pd.read_csv(\"/kaggle/input/clan-complete/CLAN_data_cleaned.csv\")\n",
    "df = df[[\"Social Media Post\", \"Normalized Claim\"]].dropna()\n",
    "\n",
    "# Split the dataset: 70% train, 15% validation, 15% test\n",
    "train_df, temp_df = train_test_split(df, test_size=0.30, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Load BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"Social Media Post\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples[\"Normalized Claim\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    labels = targets[\"input_ids\"]\n",
    "    labels = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in label_seq]\n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Load model and device\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Load evaluation metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "            # Generate predictions\n",
    "            generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=64)\n",
    "\n",
    "            # Replace -100 in labels before decoding\n",
    "            labels = torch.where(labels != -100, labels, torch.tensor(tokenizer.pad_token_id).to(labels.device))\n",
    "\n",
    "            # Decode\n",
    "            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "            predictions.extend(decoded_preds)\n",
    "            references.extend(decoded_labels)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    rouge_result = rouge.compute(predictions=predictions, references=references, rouge_types=[\"rougeL\"])\n",
    "    bleu_result = bleu.compute(predictions=predictions, references=references)\n",
    "    bertscore_result = score(predictions, references, lang=\"en\", verbose=False)\n",
    "    bertscore_avg = bertscore_result[2].mean().item()\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"Train Loss : {avg_train_loss:.4f}\")\n",
    "    print(f\"Val Loss   : {avg_val_loss:.4f}\")\n",
    "    print(f\"ROUGE-L    : {rouge_result['rougeL']:.4f}\")\n",
    "    print(f\"BLEU-4     : {bleu_result['bleu']:.4f}\")\n",
    "    print(f\"BERTScore  : {bertscore_avg:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        model.save_pretrained(\"/kaggle/working//bart_model_output/final\")\n",
    "        tokenizer.save_pretrained(\"/kaggle/working//bart_model_output/final\")\n",
    "        print(\"Best model saved.\")\n",
    "\n",
    "# Save the test split for inference\n",
    "test_df.to_csv(\"/kaggle/working/test_data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5608b6",
   "metadata": {},
   "source": [
    "# INFERENCE PIPELINE (BART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff0076f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T22:24:04.519254Z",
     "iopub.status.busy": "2025-04-08T22:24:04.518834Z",
     "iopub.status.idle": "2025-04-08T22:25:38.660961Z",
     "shell.execute_reply": "2025-04-08T22:25:38.660061Z",
     "shell.execute_reply.started": "2025-04-08T22:24:04.519221Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6edf624c8244307bb01539e55e36e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation:\n",
      "ROUGE-L : 0.3314\n",
      "BLEU-4  : 0.2220\n",
      "BERTScore (F1): 0.8822\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from bert_score import score\n",
    "\n",
    "# Load saved model and tokenizer\n",
    "model_path = \"/kaggle/input/t5_small/pytorch/default/1/bart_model_output/final\"                                # CHNAGED WITH SAVE MODELS PATH\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv(\"/kaggle/working/test_data_cleaned.csv\")       # CHANGED WITH PROCESSED TEST DATA's PATH\n",
    "test_df = test_df[[\"Social Media Post\", \"Normalized Claim\"]].dropna()\n",
    "\n",
    "# Tokenize test data\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"Social Media Post\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples[\"Normalized Claim\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    labels = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in label_seq]\n",
    "        for label_seq in targets[\"input_ids\"]\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "        \"label_ids\": targets[\"input_ids\"]  # Keep original label IDs for decoding\n",
    "    }\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"label_ids\"])\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# DataLoader for test set\n",
    "test_loader = DataLoader(tokenized_test_dataset, batch_size=1)\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# Inference loop\n",
    "for batch in test_loader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(batch[\"label_ids\"], skip_special_tokens=True)\n",
    "\n",
    "    predictions.extend(decoded_preds)\n",
    "    references.extend(decoded_labels)\n",
    "\n",
    "# Evaluation\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references, rouge_types=[\"rougeL\"])\n",
    "bleu_result = bleu.compute(predictions=predictions, references=references)\n",
    "bertscore_result = score(predictions, references, lang=\"en\", verbose=False)\n",
    "bertscore_avg = bertscore_result[2].mean().item()\n",
    "\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "print(f\"ROUGE-L : {rouge_result['rougeL']:.4f}\")\n",
    "print(f\"BLEU-4  : {bleu_result['bleu']:.4f}\")\n",
    "print(f\"BERTScore (F1): {bertscore_avg:.4f}\")\n",
    "\n",
    "# Optional: Save predictions to CSV\n",
    "output_df = pd.DataFrame({\n",
    "    \"Original Post\": test_df[\"Social Media Post\"],\n",
    "    \"Reference Claim\": references,\n",
    "    \"Predicted Claim\": predictions\n",
    "})\n",
    "output_df.to_csv(\"/kaggle/working/bart_test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec07fa75",
   "metadata": {},
   "source": [
    "# Code to train T5-Small Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d59b2-5575-4ed9-b1bf-6bbb139d167d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y transformers\n",
    "!pip install transformers==4.51.1 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "875106d0-0393-4ff3-985d-54efc0253699",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T22:04:14.747699Z",
     "iopub.status.busy": "2025-04-08T22:04:14.747403Z",
     "iopub.status.idle": "2025-04-08T22:04:17.195136Z",
     "shell.execute_reply": "2025-04-08T22:04:17.194093Z",
     "shell.execute_reply.started": "2025-04-08T22:04:14.747674Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9f1bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T22:04:19.666127Z",
     "iopub.status.busy": "2025-04-08T22:04:19.665548Z",
     "iopub.status.idle": "2025-04-08T22:04:28.240250Z",
     "shell.execute_reply": "2025-04-08T22:04:28.238934Z",
     "shell.execute_reply.started": "2025-04-08T22:04:19.666090Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tesla T4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5de4dc7a389485480fe87ae61539861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/330 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4073d62892614bbcb74072df7b300dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad867250f5a64243afa22fefc10863bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/71 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b3ee5df03cd2>\u001b[0m in \u001b[0;36m<cell line: 111>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# --- 5. Training ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./t5_clan\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mper_device_train_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration, \n",
    "    Trainer, TrainingArguments, \n",
    "    DataCollatorForSeq2Seq, TrainerCallback\n",
    ")\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bertscore\n",
    "\n",
    "# --- 0. Check for GPU ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not available.\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "df = pd.read_csv(\"/kaggle/input/clan-complete/CLAN_data_cleaned_test.csv\")\n",
    "df = df[[\"Social Media Post\", \"Normalized Claim\"]].dropna().drop_duplicates()\n",
    "df = df[(df[\"Social Media Post\"].str.strip() != \"\") & (df[\"Normalized Claim\"].str.strip() != \"\")]\n",
    "\n",
    "train_df = df.sample(frac=0.7, random_state=42)\n",
    "temp_df = df.drop(train_df.index)\n",
    "val_df = temp_df.sample(frac=0.5, random_state=42)\n",
    "test_df = temp_df.drop(val_df.index)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_ds = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# --- 2. Tokenizer and Model ---\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "\n",
    "# --- 3. Preprocessing ---\n",
    "def preprocess(example):\n",
    "    input_enc = tokenizer(\n",
    "        \"normalize: \" + example[\"Social Media Post\"],\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    target_enc = tokenizer(\n",
    "        example[\"Normalized Claim\"],\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    input_enc[\"labels\"] = [\n",
    "        (label if label != tokenizer.pad_token_id else -100)\n",
    "        for label in target_enc[\"input_ids\"]\n",
    "    ]\n",
    "    return input_enc\n",
    "\n",
    "train_ds = train_ds.map(preprocess)\n",
    "val_ds = val_ds.map(preprocess)\n",
    "test_ds = test_ds.map(preprocess)\n",
    "\n",
    "train_ds.set_format(type=\"torch\")\n",
    "val_ds.set_format(type=\"torch\")\n",
    "test_ds.set_format(type=\"torch\")\n",
    "\n",
    "# --- 4. Evaluation Callback ---\n",
    "class EvalMetricsCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model.eval()\n",
    "        predictions, references = [], []\n",
    "\n",
    "        for i in range(min(100, len(val_ds))):\n",
    "            sample = val_ds[i]\n",
    "            input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "            attn_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "            labels = sample[\"labels\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                gen_ids = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attn_mask,\n",
    "                    max_length=128,\n",
    "                    num_beams=4\n",
    "                )\n",
    "\n",
    "            pred = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "            ref = tokenizer.decode([t for t in labels if t != -100], skip_special_tokens=True)\n",
    "\n",
    "            predictions.append(pred)\n",
    "            references.append(ref)\n",
    "\n",
    "        # Compute BLEU-4\n",
    "        bleu_scores = [\n",
    "            sentence_bleu([ref.split()], pred.split(), smoothing_function=SmoothingFunction().method1)\n",
    "            for pred, ref in zip(predictions, references)\n",
    "        ]\n",
    "        print(f\"\\nEpoch {int(state.epoch)} Metrics:\")\n",
    "        print(\"BLEU-4    :\", round(sum(bleu_scores) / len(bleu_scores), 4))\n",
    "\n",
    "        # ROUGE-L\n",
    "        rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        rouge_scores = [rouge.score(r, p)['rougeL'].fmeasure for p, r in zip(predictions, references)]\n",
    "        print(\"ROUGE-L   :\", round(sum(rouge_scores) / len(rouge_scores), 4))\n",
    "\n",
    "        # BERTScore\n",
    "        P, R, F1 = bertscore(predictions, references, lang=\"en\", verbose=False)\n",
    "        print(\"BERTScore :\", round(F1.mean().item(), 4))\n",
    "\n",
    "# --- 5. Training ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_clan\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=5,\n",
    "    do_eval=True,\n",
    "    do_train=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EvalMetricsCallback()]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"/kaggle/working/t5_clan\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/t5_clan\")\n",
    "\n",
    "# --- 6. Predict One Sample ---\n",
    "sample = test_ds[0]\n",
    "input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "attn_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids=input_ids, attention_mask=attn_mask, max_length=128)\n",
    "\n",
    "print(\"\\nPrediction Example:\")\n",
    "print(\"Input     :\", tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True))\n",
    "print(\"Prediction:\", tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "print(\"Reference :\", tokenizer.decode([i for i in sample[\"labels\"] if i != -100], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ecd7f9",
   "metadata": {},
   "source": [
    "# INFERENCE PIPELINE (T5-Small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e3b646d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T22:17:33.035326Z",
     "iopub.status.busy": "2025-04-08T22:17:33.034939Z",
     "iopub.status.idle": "2025-04-08T22:20:28.462080Z",
     "shell.execute_reply": "2025-04-08T22:20:28.461101Z",
     "shell.execute_reply.started": "2025-04-08T22:17:33.035299Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 343/343 [02:44<00:00,  2.08it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Test Set:\n",
      "BLEU-4    : 0.1731\n",
      "ROUGE-L   : 0.3564\n",
      "BERTScore : 0.8815\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Imports ---\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bertscore\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# --- 2. Load Test Data ---\n",
    "df = pd.read_csv(\"/kaggle/working/test_data_cleaned.csv\")\n",
    "df = df[[\"Social Media Post\", \"Normalized Claim\"]].dropna().drop_duplicates()\n",
    "df = df[(df[\"Social Media Post\"].str.strip() != \"\") & (df[\"Normalized Claim\"].str.strip() != \"\")]\n",
    "\n",
    "\n",
    "# --- 3. Load Trained Model and Tokenizer ---\n",
    "model_path = \"/kaggle/input/t5_small/pytorch/default/1/t5_clan\"  # same as used in training\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# --- 4. Inference Function ---\n",
    "def generate_prediction(text):\n",
    "    input_text = \"normalize: \" + text\n",
    "    encodings = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "    input_ids = encodings[\"input_ids\"].to(device)\n",
    "    attn_mask = encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.generate(input_ids=input_ids, attention_mask=attn_mask, max_length=128, num_beams=4)\n",
    "\n",
    "    return tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# --- 5. Predict on Test Data ---\n",
    "predictions = []\n",
    "for text in tqdm(df[\"Social Media Post\"]):\n",
    "    pred = generate_prediction(text)\n",
    "    predictions.append(pred)\n",
    "\n",
    "df[\"Predicted Claim\"] = predictions\n",
    "\n",
    "\n",
    "# --- 6. Evaluation Metrics ---\n",
    "references = df[\"Normalized Claim\"].tolist()\n",
    "preds = df[\"Predicted Claim\"].tolist()\n",
    "\n",
    "# BLEU-4\n",
    "bleu_scores = [\n",
    "    sentence_bleu([ref.split()], pred.split(), smoothing_function=SmoothingFunction().method1)\n",
    "    for ref, pred in zip(references, preds)\n",
    "]\n",
    "bleu4 = round(sum(bleu_scores) / len(bleu_scores), 4)\n",
    "\n",
    "# ROUGE-L\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "rouge_scores = [rouge.score(ref, pred)['rougeL'].fmeasure for ref, pred in zip(references, preds)]\n",
    "rougeL = round(sum(rouge_scores) / len(rouge_scores), 4)\n",
    "\n",
    "# BERTScore\n",
    "_, _, f1 = bertscore(preds, references, lang=\"en\", verbose=False)\n",
    "bert_score = round(f1.mean().item(), 4)\n",
    "\n",
    "\n",
    "# --- 7. Output Results ---\n",
    "print(\"\\nFinal Evaluation on Test Set:\")\n",
    "print(\"BLEU-4    :\", bleu4)\n",
    "print(\"ROUGE-L   :\", rougeL)\n",
    "print(\"BERTScore :\", bert_score)\n",
    "\n",
    "# Save predictions to CSV\n",
    "df.to_csv(\"T5_Test_Predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2454a479-5085-45ba-9f2f-e7bfbc41165a",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7084456,
     "sourceId": 11326009,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 296453,
     "modelInstanceId": 275558,
     "sourceId": 328392,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
